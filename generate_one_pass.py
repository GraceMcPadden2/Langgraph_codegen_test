import pandas as pd
from collections import defaultdict
from typing import Dict, Annotated, List, TypedDict
from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
import os,sys, subprocess


load_dotenv()
# ---------- 1. Graph state ---------------------------------------------------
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]


# ---------- 2. Tool ----------------------------------------------------------
@tool
def convert_data(file_path: str) -> pd.DataFrame:
    """takes in a file path to an excel file, returns a pandas data frame

    Args:
        file_path (str): path to local file containing spreasheet

    Returns:
        pd.DataFrame: pandas data frame
    """
    return pd.read_excel(file_path)

@tool
def save_doc(script):
    """takes in the script generated by agent, saves to a local file

    Args:
        script (string): code generated

    Returns:
        none
    """
    with open("script.py", "a") as file:
        file.write(script)


tools = [convert_data, save_doc]


# ---------- 3. LLM bound to tools -------------------------------------------
llm = ChatOpenAI(model="gpt-4o").bind_tools(tools)
tool_node = ToolNode(tools)

# ---------- 4. Agent (reasoning) node ---------------------------------------
def agent_node(state: AgentState) -> AgentState:
    system_msg = SystemMessage(content="""
You are a devloper writing classes in Python. You will be given a file path to an excel file containing a spreadsheet. Always prioritze best practices and readability.

    - When the user provides a file path, You will call convert_data tool with the file path provided by the user. This tool will read the excel file and return a df of the categories.
    - You will then take the df returned by the tool, and generate Python code that defines a class for each category. Put all sections in one class.
    - Include every section in the df, do not exclude any.
                               
You will not run the code, just generate it and save it to a file.
                        
""")

    # prepend system instruction each turn
    
    response = llm.invoke([system_msg] + state["messages"])

    if response.content:
        print(f"\nAI: {response.content}\n")
    elif response.tool_calls:
        print(f"\nTool call: {response.tool_calls}\n")
    else:
        print("\n No content or tool call returned.\n")

    state["messages"].append(response)
    return state

# ---------- 5. Stop/continue logic ------------------------------------------
def should_continue(state: AgentState) -> str:
    last = state["messages"][-1]

    # If the AI queued a tool call, let the ToolNode run next
    if isinstance(last, AIMessage) and last.tool_calls:
        return "continue"

    # Otherwise weâ€™re finished
    return "end"

# ---------- 6. Build graph ---------------------------------------------------
graph = StateGraph(AgentState)
graph.add_node("agent", agent_node)
graph.add_node("tool", tool_node)

graph.set_entry_point("agent")
graph.add_conditional_edges("agent", should_continue,
                            {"continue": "tool", "end": END})
graph.add_edge("tool", "agent")

agent = graph.compile()

# ---------- 7. Chat loop -----------------------------------------------------
history: List[BaseMessage] = []

while True:
    user = input("Please enter file path: ")
    if user.lower() == "exit":
        break
    history.append(HumanMessage(content=user))
    result = agent.invoke({"messages": history})
    history = result["messages"]